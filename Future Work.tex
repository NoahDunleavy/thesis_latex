This work is by no means complete. There are plenty of ideas I wish to pursue in the future.

The first item to attack more rigorously and quantify are the trade-offs between $Q$, $R$, $\Phi_u$, $\Phi_y$, and $v(k)$ values. For many examples, it was necessary to drastically increase $R$ to show that the RL framework continued to function, but obviously increased the learning time necessary for a given problem. Exploring ways to reduce $R$ without increasing the necessary exploration terms would drastically improve practicality of learning a controller. Especially in an ILC manufacturing scenario, if one could learn without purposely messing up parts, this would be extremely attractive. \myworries{Scaling issue to improve numerical conditioning}

Another idea to pursue is to expand on the final example shown in Section~\ref{sec:rl_on_conjugate_basis_ilc}. 
Or rather - reduce it. Instead of constructing a lower-triangular matrix, imagine a scenario where one builds a diagonal controller. Now, each controller could be updated in $4$ trials when a new basis function comes along. If this formulation worked (with a sufficiently small $R$), then it would be possible to find a controller for a complete system in $p(4 + 1)$ trials - $4$ trials for the RL approach and $1$ to generate the associated conjugate basis function.
\myworries{Left and right signular vectors. $U^T$y = S$V^T$u mapping is diagonal}
\myworries{This theory is not limitted to linear tiem invariant - works for linear time variant which can be used to model non linear systems}

It would additionally be interesting to figure out what the learned $\underline{u}$ is when the $\Phi_u$ does not capture the $\underline{u}^\ast$. It has been shown to not be the projection of $\underline{u}^\ast$. This is likely due to the fact that the $\underline{u}$ is transformed by the $P$ matrix, and then looped through $\Phi_y^+$ conversions before going back through a controller $\mathcal{L}$ to get $\beta$s (or $\delta \beta$s) all before going back to being a $\underline{u}$.

Finally, we have investigated initial steps to reduce dimensions then grow, but no techniques to then work backwards. As shown in Eq.~\ref{eq:conjugate_beta_star}, it is possible to learn the weighting of some inputs even when they were not in the original $\underline{u}^\ast$. It would be desirable to see if there were a way to work backwards to remove certain basis functions, or combine them in with other functions. Take the basis space $\Phi$
\begin{equation}
    \Phi = 
    \begin{bmatrix}
        \phi_1 & \phi_2 & \cdots & \phi_{b-1} & \phi_{b}
    \end{bmatrix}
\end{equation}
that previously satisfied our conjunctionality condition such that
\begin{equation}
    \Phi^T (R + P^T Q P) \Phi = I_{b \times b}
\end{equation}
If we were to combine $\phi_{b-1}$ and $\phi_{b}$ into one term, we could reduce the dimension of our basis space to created $\Phi^\prime$ but then our conjunct condition is violated. Now
\begin{equation}
    {\Phi^\prime}^T (R + P^T Q P) \Phi^\prime 
    = 
    \begin{bmatrix}
        I_{(b-2) \times (b-2)} & 0 \\
        0 & 2
    \end{bmatrix}
\end{equation}
but if we were to sum together all our $\phi$s such that each basis function had a pair, trio, or some equal number of functions being combined into it, then we would still have our conjunct condition - just scaled. Perhaps one could even take one function, then divide it out among the remain functions to maintain the scaled $I$ for the conjunct condition. The feasibility of this approach is something worth pursing I believe.
