\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Reinforcement Learning on Iterative Learning Control}{57}{section.2.1}\protected@file@percent }
\newlabel{sec:rl_on_ilc}{{2.1}{57}{Reinforcement Learning on Iterative Learning Control}{section.2.1}{}}
\newlabel{eq:ilc_utility}{{2.1}{57}{Reinforcement Learning on Iterative Learning Control}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Example --- Policy Iteration on ILC}{57}{subsection.2.1.1}\protected@file@percent }
\newlabel{eq:rl_ilc_params}{{2.2}{58}{Example --- Policy Iteration on ILC}{equation.2.2}{}}
\newlabel{eq:ilc_lqr_controller}{{2.3}{58}{Example --- Policy Iteration on ILC}{equation.2.3}{}}
\newlabel{eq:rl_ilc_input}{{2.4}{58}{Example --- Policy Iteration on ILC}{equation.2.4}{}}
\newlabel{eq:rl_ilc_policy_controller}{{2.13}{60}{Example --- Policy Iteration on ILC}{equation.2.13}{}}
\newlabel{eq:controller_error_calc}{{2.14}{60}{Example --- Policy Iteration on ILC}{equation.2.14}{}}
\newlabel{fig:policy_ilc_F1}{{2.1a}{61}{\relax }{figure.caption.48}{}}
\newlabel{sub@fig:policy_ilc_F1}{{a}{61}{\relax }{figure.caption.48}{}}
\newlabel{fig:policy_ilc_F20}{{2.1d}{61}{\relax }{figure.caption.48}{}}
\newlabel{sub@fig:policy_ilc_F20}{{d}{61}{\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Select Controller Weights on Select Inputs for an ILC problem through Policy Iteration Trials. 5 Controllers are learned, for an ILC system on the Dual-Spring-Mass system of trial length $p=10$ -- each controller update requires 1600 ILC trials}}{61}{figure.caption.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Error Magnitude of Output through Policy Iteration Trials, where $Q/R = 100$. Observe that after the first controller is learned and applied, there is a sharp reduction in error but due to the relatively high $R$ in its definition, subsequent reductions in error are much slower.}}{61}{figure.caption.49}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Input-Output Progressions through Policy Iteration Trials of $Q/R=100$. Observe Trial 1 to be the open-loop response, and subsequent trials to be progressing towards the goal output.}}{62}{figure.caption.50}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Shaped Output through Policy Iteration Trials of $Q/R=100$. Observe the progression towards the desired output, but also the extreme number of trials that would be needed to properly further the learning.}}{63}{figure.caption.51}\protected@file@percent }
\newlabel{eq:rl_ilc_lqr_small_R_controller}{{2.16}{65}{Example --- Policy Iteration on ILC}{equation.2.16}{}}
\newlabel{eq:rl_ilc_policy_small_R_controller}{{2.17}{65}{Example --- Policy Iteration on ILC}{equation.2.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Select Controller Weights on Select Inputs progression through Policy Iteration Trials when $Q/R = 1\times 10^8$. Due to the necessary amplified exploration $v(k)$, observe the tendency for weights to converge much less smoothly than before, displaying behaviors of overshoot and lag.}}{66}{figure.caption.52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Error Magnitude of Output through Policy Iteration Trials with $Q/R = 1\times 10^8$. Observe how initial errors creep up much further than the earlier shown $Q/R=100$ trial, but the application of the first trial drastically reduces error , such that it would be zero were it not for the extreme input exploration terms.}}{66}{figure.caption.53}\protected@file@percent }
\newlabel{fig:reduced_R_ilc_error}{{2.6}{66}{Error Magnitude of Output through Policy Iteration Trials with $Q/R = 1\times 10^8$. Observe how initial errors creep up much further than the earlier shown $Q/R=100$ trial, but the application of the first trial drastically reduces error , such that it would be zero were it not for the extreme input exploration terms}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Progression of Input-Output Data in a Dual-Spring-Mass system under ILC derived from RL when $Q/R = 1\times 10^8$}}{67}{figure.caption.54}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Shaped Output through Policy Iteration Trials when $Q/R = 1\times 10^8$}}{68}{figure.caption.55}\protected@file@percent }
\newlabel{fig:shaped_rl_small_R}{{2.8}{68}{Shaped Output through Policy Iteration Trials when $Q/R = 1\times 10^8$}{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Example --- Input Decoupling on ILC}{69}{subsection.2.1.2}\protected@file@percent }
\newlabel{eq:decoupled_ilc_controller}{{2.22}{71}{Example --- Input Decoupling on ILC}{equation.2.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Select Controller Weights on Select Inputs through Input Decoupling Trials to learn the ILC Controller when $Q/R=100$. Notice how each controller update takes $441$ ILC trials, and each input controller is only updated at that rate.}}{71}{figure.caption.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Error Magnitude of Output through Input Decoupling Trials}}{72}{figure.caption.57}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Input-Output Data progression through Input Decoupled Learning trials when $Q/R = 100$}}{73}{figure.caption.58}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Shaped Output through Input Decoupling Trials when $Q/R=100$.}}{74}{figure.caption.59}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Summary of RL on ILC}{75}{subsection.2.1.3}\protected@file@percent }
\@setckpt{Reinforcement Learning on Iterative Learning Control}{
\setcounter{page}{76}
\setcounter{equation}{22}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{1}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{section@level}{2}
\setcounter{Item}{0}
\setcounter{Hfootnote}{7}
\setcounter{bookmark@seq@number}{26}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{lstlisting}{0}
}
