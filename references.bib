@inproceedings{PhanFrueh1996,
  author={Phan, M.Q. and Frueh, J.A.},
  booktitle={Proceedings of 35th IEEE Conference on Decision and Control}, 
  title={Learning control for trajectory tracking using basis functions}, 
  year={1996},
  volume={3},
  number={},
  pages={2490-2492 vol.3},
  keywords={Trajectory;Aerodynamics;Time varying systems;Control systems;Process control;Iterative methods;Learning systems;Error correction;Electrical equipment industry;PD control},
  doi={10.1109/CDC.1996.573465}
}

@inproceedings{FruehPhan1998,
  author={Frueh, J.A. and Phan, M.Q.},
  booktitle={Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171)}, 
  title={Linear quadratic optimal learning control (LQL)}, 
  year={1998},
  volume={1},
  number={},
  pages={678-683 vol.1},
  keywords={Optimal control;Cost function;Control system synthesis;Trajectory;Control systems;Aerospace engineering;History;Data mining;Iterative algorithms;Automatic control},
  doi={10.1109/CDC.1998.760762}
}

@unpublished{Phan2025,
  author = {Phan, M.Q.},
  note = {Writing in Process},
  title = {Optimal State-Space System Identification and Control},
  year = {}
}

@ARTICLE{9374773,
  author={Saab, Samer Said and Shen, Dong and Orabi, Mohamad and Kors, David and Jaafar, Rayana H.},
  journal={IEEE Transactions on Industrial Electronics}, 
  title={Iterative Learning Control: Practical Implementation and Automation}, 
  year={2022},
  volume={69},
  number={2},
  pages={1858-1866},
  keywords={Trajectory;Robots;Convergence;Position measurement;Adaptive control;Process control;Task analysis;Industrial robot manipulator;initial state mismatch;iterative learning control (ILC);proportional–integral–derivative (PID) control},
  doi={10.1109/TIE.2021.3063866}}

  @INPROCEEDINGS{4178138,
  author={Moore, Kevin L. and Chen, YangQuan and Ahn, Hyo-Sung},
  booktitle={Proceedings of the 45th IEEE Conference on Decision and Control}, 
  title={Iterative Learning Control: A Tutorial and Big Picture View}, 
  year={2006},
  volume={},
  number={},
  pages={2352-2357},
  keywords={Tutorial;Convergence;Robust control;Humans;Robots;Inductors;Hard disks;Servomechanisms;Learning systems;Control systems;Iterative learning control;monotonic convergence;super-vector framework;iteration-variant robustness},
  doi={10.1109/CDC.2006.377582}}

  @ARTICLE{1636313,
  author={Bristow, D.A. and Tharayil, M. and Alleyne, A.G.},
  journal={IEEE Control Systems Magazine}, 
  title={A survey of iterative learning control}, 
  year={2006},
  volume={26},
  number={3},
  pages={96-114},
  keywords={Iterative methods;Learning systems;Control systems;Open loop systems;Error correction;Radio control;Adaptive control;Chemical industry;Muscles;Motion control},
  doi={10.1109/MCS.2006.1636313}}

  @article{ZHANG2019314,
title = {A Preliminary Study on the Relationship Between Iterative Learning Control and Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {29},
pages = {314-319},
year = {2019},
note = {13th IFAC Workshop on Adaptive and Learning Control Systems ALCOS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.669},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319326187},
author = {Yueqing Zhang and Bing Chu and Zhan Shu},
keywords = {Iterative learning control, reinforcement learning},
abstract = {Iterative learning control is a control system design method that is able to achieve high tracking performance by repeatedly executing a task and learning the best input from previous attempts of performing the task. Reinforcement learning is a machine learning method that determines the best action such that some utility function (reward) is maximised by repeatedly interacting with the environment (system) and learning the best action policy based on the reward received from such interactions. These two methods belong to different subject disciplines but share a number of similarities. The relationship between these two design approaches, however, has not been investigated in detail. This paper presents a preliminary study on the relationship between iterative learning control and reinforcement learning, hopefully shedding some light on how these two areas can benefit each other in future research.}
}







@article{doi:10.2514/1.I010037,
author = {Nguyen, Nhan},
title = {Least-Squares Model-Reference Adaptive Control with Chebyshev Orthogonal Polynomial Approximation},
journal = {Journal of Aerospace Information Systems},
volume = {10},
number = {6},
pages = {268-286},
year = {2013},
doi = {10.2514/1.I010037},

URL = { 
        https://doi.org/10.2514/1.I010037
},
eprint = { https://doi.org/10.2514/1.I010037}
,
    abstract = { This paper presents a model-reference adaptive control approach for systems with unstructured uncertainty based on two least-squares parameter estimation methods: gradient-based method and recursive least-squares method. The unstructured uncertainty is approximated by Chebyshev orthogonal polynomial basis functions. The use of orthogonal basis functions improves the function approximation significantly and enables better convergence of parameter estimates. The least-squares gradient adaptive control achieves superior parameter convergence as compared to the standard model-reference adaptive control. Flight control simulations were conducted with four adaptive controllers: least-squares gradient adaptive control, recursive least-squares adaptive control, standard model-reference adaptive control, and neural-network adaptive control. The results show that the recursive least-squares adaptive control achieves better robustness as measured by a time-delay margin, while the least-squares gradient adaptive control achieves better tracking performance than both the standard model-reference adaptive control and neural-network adaptive control. }
}









