%Warnings to suppress (manually decide for each file)
% chktex-file 8 %suppress warning about dash lengths
% chktex-file 36 %suppress warning about spaces and ()

\FloatBarrier\section{Basis Functions}
~\label{sec:basis_functions}
While we have shown that the RL process still works for the ILC problem, the dimensional limits present a significant problem. Signal dimension reduction is not a new challenge, and can be seen from Fourier Transforms to Linear Regressions. The method which we will explore are Basis Functions. \cite{PhanFrueh1996} - Learning Control for Trajectory Tracking using Basis Functions - provides the logic from which we launch our approach. 

\FloatBarrier\subsection{What are Basis Functions}
~\label{sub:what_are_basis}
Basis Functions offer the ability to represent a signal as a weighting of composite signals. Suppose you had the vector
\begin{equation}
    \begin{bmatrix}
        -2 & 8 & 13 & -4 & 9
    \end{bmatrix}^T
    \label{eq:example_vector_basis}
\end{equation}
You could express this as a sequence of five numbers, or if you had already pre-defined some vector $\phi$ as
\begin{equation}
    \phi = 
    \begin{bmatrix}
        1 & -4 & -6.5 & 2 & -4.5
    \end{bmatrix}^T
\end{equation}
you could capture Eq.~\ref{eq:example_vector_basis} exactly as $-2\phi$. 

This is the premise behind basis functions. For any signal of length (or `resolution') $\ell$, we can describe it as a composite of $\eta$ functions that are defined for $\ell$ points. We then create a $\ell \times \eta$ `basis space' $\Phi$ out of basis functions $\phi_i$, as shown in Eq.~\ref{eq:basis_space_phi}
\begin{equation}
    \Phi = 
    \begin{bmatrix}
        | & | &  & | \\
        \phi_1 & \phi_2 & \cdots & \phi_{\eta} \\
        | & | &  & | \\
    \end{bmatrix}
    \label{eq:basis_space_phi}
\end{equation}
where each $\phi_i$ is a ($\ell \times 1$) vector. The only condition on each basis functions is that it is independent of / orthogonal to any other basis functions.
\begin{equation}
    \phi_i \cdot \phi_j = 0,\ \text{for}\ i \ne j
    \label{eq:orthogonal_basis}
\end{equation}
where $\cdot$ is the dot operator. Put another way, $\Phi$ must be full rank. 

To represent a signal in terms of basis functions, we then use a $\eta \times 1$ weighting vector $\beta$. 
\begin{equation}
    \beta = 
    \begin{bmatrix}
        \beta_1 \\ \beta_2 \\ \vdots \\ \beta_\eta
    \end{bmatrix}
\end{equation}
In the ideal scenario, a signal can be captured with a single $\beta$. This happens when the signal we are representing is a scalar of the chosen basis function, as shown in Eq.~\ref{eq:example_vector_basis}. The worst case scenario we have also already seen -- you just may not have realized it. Whenever a basis space is not specified, and a signal still represented, a $\ell \times \ell$ identity matrix is implicitly being used, and thus $\ell$ basis functions. If we were to re-express Eq.~\ref{eq:example_vector_basis} in the worst case, then it would at most take five basis weights
\begin{equation}
    \begin{bmatrix}
        -2\\ 8\\ 13\\ -4\\ 9
    \end{bmatrix}
    =
    \begin{bmatrix}
        1 & 0 & 0 & 0 & 0\\0 & 1 & 0 & 0 & 0\\ 0 & 0 & 1 & 0& 0 \\ 0 & 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & 1
    \end{bmatrix}
    \begin{bmatrix}
        -2\\ 8\\ 13\\ -4\\ 9
    \end{bmatrix}
\end{equation}
The Basis Space also does not necessarily need to be $I_{\ell \times \ell}$. So long as it is full rank, a basis space of $\ell$ basis functions can perfectly capture any signal. An added feature of the basis space as we have defined it, is its width will never exceed its height. That is, it will always be a tall matrix (square at most), as $\eta \le \ell$. This fact, coupled with the full rank nature, ensures that the a left-side product between the space's pseudo-inverse and itself will always be the identity matrix.

\begin{equation}
    I_{\eta \times \eta} = \Phi^+ \Phi
    \label{eq:identity_of_basis_spaces}
\end{equation}

To summarize, any $\ell \times 1$ signal $\omega$ can be expressed exactly as a product of a basis space $\Phi$ and basis coefficients $\beta$, so long as $\omega$ exists in the basis space.

\begin{equation}
    \omega = \Phi \beta
    \label{eq:basis_definition}
\end{equation}
The trick then comes down to picking the right basis functions, and ensuring $\omega$ is in the basis space. With luck, it would be possible to get a signal with a single function and weight, but luck is never a good plan.

\FloatBarrier\subsection{Chebyshev Polynomials}
Chebyshev Polynomials offer a logical foundation and algorithm for constructing basis functions\footfullcite{doi:10.2514/1.I010037}. Chebyshev Polynomials of the first kind, $T_n$ are defined as
\begin{equation}
    T_n(\cos \theta) = \cos(n\theta)
    \label{eq:full_cheby_def}
\end{equation}
and have a useful property that they are all orthogonal with respect to one another, defined over the space of [-1, 1]. They can also be generated iteratively in recurrence equation shown in Eq.~\ref{eq:cheby_recurrence}
\begin{align}
    T_0(x) &= 1 \label{eq:cheby_T0}\\
    T_1(x) &= x \label{eq:cheby_T1}\\
    T_{n+1}(x) &= 2xT_n(x) - T_{n-1}(x)\label{eq:cheby_recurrence}
\end{align}

Earlier functions, due to their lower frequency, help capture macro behaviors in a signal. As one progresses further in the process, the higher frequency signals capture the finer details. To those familiar with Fourier Analysis, you will recognize this behavior of marginal enhancements with each additional, higher frequency. Or it is not unlike in a Taylor Expansion, where we similarly see diminishing returns on accuracy from each additional component added. 

\FloatBarrier\subsubsection{Example --- Matlab Creation of Chebyshev Polynomials}

To utilize the features of Chebyshev Polynomials, we must first create them. Since they are only orthogonal over the domain of $x \in [-1, 1]$, that is where we will restrict ourselves for their generation. Recalling we wish to capture a signal of $\ell$ points, we will then define our $x$ from $-1 \rightarrow 1$ in steps of $\frac{2}{\ell}$. This can be accomplished in Matlab by setting our $x$ to \texttt{x = linspace(-1, 1, $\ell$)'}\footnote{Note the ' to transpose the vector into the column format} -- this is our $T_1$ (Eq.~\ref{eq:cheby_T1}). We must similarly create a $\ell \times 1$ of all 1s to serve as our $T_0$ (Eq.~\ref{eq:cheby_T0}). To generate $\eta$ functions, we then iterate as described in Eq.~\ref{eq:cheby_recurrence}. The process can also be seen in Code Appendix~\ref{code:gen_cheby}

In the presented example, we set $\ell=100$ and $\eta = 20$, generating a $\Phi$ that is $100 \times 20$. Figure~\ref{fig:example_chebys} shows our $T_0$, $T_1$, and some select iteratively generated functions. You will see that the x-axis units are `Chebyshev Steps' that go from $0 \rightarrow 99$, even though we numerically said they had to go from $-1 \to 1$. Numerically the functions must be defined in this range, but for application the can be viewed purely as data points along a signal of any length, and the real values of $-1$ and $1$ no longer hold any meaning.
\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{{Basis Functions/Example Cheby Functions.pdf}} %figure is as wide as text
    \caption{Select Chebyshev Polynomials}
~\label{fig:example_chebys}
\end{figure}

We can now utilize these functions to generate a wide array of complex signals. To demonstrate the arbitrary capabilities, refer to Figure~\ref{fig:example_cheby_signal}. This is a 100 time step signal created with just 20 chebyshev polynomials, weighted as shown in Figure~\ref{fig:example_cheby_weights}
\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{{Basis Functions/Example Cheby Signal.pdf}} 
    \caption{Example Signal constructed from $\eta = 20$ Chebyshev Polynomials of length $\ell = 100$}
~\label{fig:example_cheby_signal}
\end{figure}

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=1\textwidth]{{Basis Functions/Cheby Weights.pdf}} 
    \caption{Example Chebyshev Weights to Generate Signal in Fig.~\ref{fig:example_cheby_signal}}
~\label{fig:example_cheby_weights}
\end{figure}

\FloatBarrier\subsection{Basis Requirements in Reinforcement Learning}

Clearly Chebyshev Polynomials are a powerful tool, but how precise must we be to apply them to our RL problem? Our goal is to reduce the number of dimensions for both the states and the inputs, so let us start by redefining the ILC problem in a basis space. Recall our model:
\begin{equation}
	\underline{y} = P\underline{u} + \underline{d}
    \tag{\ref{eq:y_Pu_d}}\\
\end{equation}

For complete flexibility, we will assign separate basis spaces to the output ($\underline{y}$) and our inputs ($\underline{u}$). The $\underline{y}$ basis space will be $\Phi_y$ and the $\underline{u}$ will be $\Phi_u$. The $\underline{u}$ will get to keep the basis coefficients convention of $\beta$ shown in Eq.~\ref{eq:basis_definition}, but the $\underline{y}$ space will be expressed in terms of $\alpha$ weights. We will have $\eta_y$ basis coefficients on the output, and $\eta_u$ basis coefficients on the input. So $\Phi_y$ will be $n_{ILC} \times \eta_y$ and $\Phi_u$ will be $r_{ILC} \times \eta_u$. 

Assuming our basis spaces to be well defined, we then have the exact relationships
\begin{align}
    \underline{y} &= \Phi_y \alpha  \label{eq:y_Ty_a} \\
    \underline{u} &= \Phi_u \beta   \label{eq:u_Tu_b}
\end{align}
which, due to the tall nature of the basis spaces, can be reversed as
\begin{align}
    \alpha &= \Phi_y^+ \underline{y}   \label{eq:a_pinv_Ty_y} \\
    \beta &= \Phi_u^+ \underline{u}    \label{eq:b_pinv_Tu_u}
\end{align}
It is important to note that when our basis spaces are not well-defined, these reverse operations become approximations. $\Phi_y^+ \underline{y}$ gives us the projection of $\underline{y}$ into the basis space defined by $\Phi_y$ -- the same goes for $\underline{u}$ and $\Phi_u$. The projection can be thought of `how much is captured' when going from one space to another. The most intuitive interpretation is to call it a shadow. Just as the shadow of a tree is telling us how much of its 3-D representation can be expressed in 2-D, the projection of $\underline{y}$ onto $\Phi_y$ tells us how much of $\underline{y}$ can be described in $\Phi_y$. However this `shadow' does not tell us if there is some data behind our proverbial tree.This allows multiple $\underline{y}$s and $\underline{u}$s to produce the same $\alpha$ and $\beta$, so long as the differences in the $\underline{y}$s and $\underline{u}$s occur in the null space (outside) of their respective basis spaces.

Substituting these identities into Eq.~\ref{eq:y_Pu_d}, we can re-write it as
\begin{equation}
    \Phi_y \alpha = P\Phi_u \beta + \underline{d}
\end{equation}
We then multiply both sides on the left by $\Phi_y^+$, and given the property in Eq.~\ref{eq:identity_of_basis_spaces}, we isolate $\alpha$ as
\begin{equation}
    \alpha = \Phi_y^+ P\Phi_u \beta + \Phi_y^+ \underline{d}
\end{equation}
To simplify matters moving forward, we introduce a new $\eta_y \times \eta_u$ matrix system dynamics $H$ which, just as $P$ captured the impact of inputs on output, will describe the impacts of $\beta$s on $\alpha$s
\begin{equation}
    H = \Phi_y^+ P\Phi_u
    \label{eq:ilc_basis_H}
\end{equation}
We then apply the $\delta$ operator and recognizing $\Phi_y^+ \underline{d}$ as a constant, drop it out as done in Eq.~\ref{eq:del_y_P_del_u}
\begin{equation}
    \delta_j \alpha = \delta_j H \beta
\end{equation}

Referring back to our section on ILC (\ref{sec:ILC}), this will all look very similar and one can properly assume our next step is to define our goal output. Recall our goal output $\underline{y}^\ast$; given the identity of Eq.~\ref{eq:a_pinv_Ty_y} we can write a new goal of $\alpha^\ast$
\begin{equation}
    \alpha^\ast = \Phi_y^+ \underline{y}^\ast
\end{equation}
meaning that each trials $\alpha_j$, now marked with the $j$ subscript to indicate trials, has an associated error. Calling this error $e_{\alpha_j}$, we have
\begin{align}
    e_{\alpha_j} &= \alpha^\ast - \alpha_j \\
    &= \Phi_y^+ (\underline{y}^\ast - \underline{y}_j)  
\end{align}
Once again applying the $\delta$ operator and following the same logical steps shown in the pure-form ILC derivation, we can write our new ILC Equation in the basis spaces $\Phi_y$ and $\Phi_u$
\begin{equation}
    e_{\alpha_{j+1}} = I e_{\alpha_{j}} - H \delta_{j+1}\beta
\end{equation}

Our new model, while still adhering to the $ABCD$ format of Eq.~\ref{eq:discrete_state_space_model} and the ILC format of Eq.~\ref{eq:ILC_law}, now has controllable dimensions. Just as going from state-space to ILC took our number of states from $n \rightarrow n_{ILC} = pn$, in the transition to basis space we have taken our state count from $n_{ILC} \rightarrow \eta_y$, where $\eta_y \le n_{ILC}$. The same can be shown for our inputs. It can also be shown at every step of the derivation process that if $\Phi_y$ and $\Phi_u$ are set to the identity matrix, we perfectly match the earlier full-dimension ILC.%

The only thing left now is to define our control law in our basis space. Recall Eq.~\ref{eq:del_u_L_e_j}
\begin{equation}
    \delta_{j+1}\underline{u}=\mathcal{L}e_j
    \tag{\ref{eq:del_u_L_e_j}}
\end{equation}
Substituting \(\delta_{j+1}\underline{u} = \delta_{j+1}\Phi_u\beta\) and \(e_j = \Phi_y e_{\alpha_j}\)
\begin{equation}
    \delta_{j+1}\Phi_u\beta = \mathcal{L} \Phi_y e_{\alpha_j}
\end{equation}
we can move the $\Phi_u$ outside the $\delta$ operator, and left-multiply both side by $\Phi_u^+$ to write
\begin{equation}
    \delta_{j+1} \beta = \Phi_u^+ \mathcal{L} \Phi_y e_{\alpha_j}
\end{equation}
Similar to how we defined $H$, we can now define the $\eta_u \times \eta_y$ controller $\mathcal{L}_\beta$
\begin{equation}
   \mathcal{L}_\beta = \Phi_u^+ \mathcal{L} \Phi_y
\end{equation}
So that
\begin{equation}
    \delta_{j+1} \beta = \mathcal{L}_\beta e_{\alpha_j}
    \label{eq:basis_ilc_control_law}
\end{equation}


Armed with this exact model and controller, we can now explore the the relationships between $\eta_y$, $\eta_u$, $\Phi_y$, $\Phi_y$, $\mathcal{L}_\beta$, $\underline{y}^\ast$ and any resultant error.

\FloatBarrier\subsubsection{Demonstration of Requirements}
Recall the assumptions (Eqs.~\ref{eq:y_Ty_a} and~\ref{eq:u_Tu_b}) we made when deriving our basis space formation.
When we called those equations `identities', that was built on the notion that the basis functions were capable of fully capturing the inputs and outputs. 
Here we wish to explore how strictly we must adhere to these assumptions.

All the following trials will employ the following parameters over 20 trials
\begin{align}
    \mathcal{L}_\beta &= 0.5H^+ \\
    p &= 100
\end{align}
We will use our perfect knowledge to ensure our controller works to control our system within a reasonable number of trials.

An important note moving forward is that some of the shaped outputs look like arbitrary nonsense. That is because they are. In examples where we define our $\underline{u}^\ast$ to set the $\underline{y}^\ast$ it is much harder to pick pleasant and recognizable images while still preserving the understandability of our theory. The presented goal `shapes' are no more arbitrary than the circle goal in Figure~\ref{fig:ilc_shaped_circle} or the word `Dartmouth' in Figure~\ref{fig:ilc_shaped_dartmouth}. 

\FloatBarrier\paragraph{{FIPO}}
~\label{par:fipo}

The first assumption we will relax is that the output basis space captures $\underline{y}^\ast$. That is our $\underline{u}^\ast$ will fully be described in $\Phi_u$, but $\underline{y}^\ast$ will only partially be in $\Phi_y$. In other words, we will have \ac{FIPO} describing their respective spaces.

We can always construct a basis space to capture $\underline{y}^\ast$ by setting one of the functions of $\Phi_y$ equal to $\underline{y}^\ast$, so this scenario could always be avoided with minimal dimensions. However we must ensure our input can be captured in our input basis space, so we construct our input basis space out of the first 10 chebyshev polynomials\footnote{Each function must be 200 points in resolution,  as our number of ILC inputs is 200 ($p \times r$)}, using the method shown in Eq.~\ref{eq:cheby_recurrence}.
\begin{equation}
    \Phi_u = 
    \begin{bmatrix}
        T_0 & T_1 & \cdots & T_8 & T_9
    \end{bmatrix}
    \label{eq:Tu_in_cheby}
\end{equation}
and we will construct $\underline{u}^\ast$ as done in Eq.~\ref{eq:u_Tu_b}, using a $\beta^\ast$ defined as
\begin{equation}
    \beta^\ast = {\begin{bmatrix}1 & 0.2 & -0.3 & 4 & 0 & 0 & 0 & -1 & 0 & 0\end{bmatrix}}^T
    \label{eq:beta_star_in_basis}
\end{equation}
which produces the input signals shown in Figures~\ref{fig:FIPO_input_1} and~\ref{fig:FIPO_input_2}. Recall that in our ILC problem, $\underline{u}$ and $\underline{y}$ are stacks of input/output data, rotating through the different components of each (see Eq.~\ref{eq:stacked_y_star}), so we must unstack them for logical interpretation. As a consequence of this `stack-to-components' action, both of the goals end up looking very similar, since they are drawn as alternating components of the same parent signal. This has no impact on the following results, as the inputs are arbitrary regardless.

\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full input Partial output - Goal Input 1.pdf}} 
        \caption{}
        \label{fig:FIPO_input_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full input Partial output - Goal Input 2.pdf}} 
        \caption{}
        \label{fig:FIPO_input_2}
    \end{subfigure}
    \caption{Goal Inputs deconstructed from an $\underline{u}$ explicitly constructed from basis functions to be in $\Phi_u$}
\end{figure}

We apply this $\underline{u}^\ast$ to our system to create $\underline{y}^\ast$. For convenience, we will set $\Phi_y$ = $\Phi_u$. It can be demonstrated that our output basis space does not capture our $\underline{y}^\ast$ by computing our $\alpha^\ast$ and attempting to go back to $\underline{y}^\ast$. For our system and given input,
\begin{align}
    \alpha^\ast &= {\Phi_y}^+ \underline{y}^\ast \tag{\ref{eq:a_pinv_Ty_y}} \\
    &= \begin{bmatrix}
        0.1364 \\
        -0.2043 \\
        0.2136 \\
        -0.2672 \\
        -0.1991 \\
        0.1067 \\
        -0.0212 \\
        -0.0241 \\
        -0.0025 \\
        -0.0320 
    \end{bmatrix}
\end{align}
If $\underline{y}^\ast$ were in $\Phi_y$, then $\underline{y}^\ast - \Phi_y \alpha^\ast$ should be a zero vector\footnote{Check this yourself using $\underline{u}^\ast$ and $\Phi_u \beta^\ast$}. However, we see both numerically in Eq.~\ref{eq:y*_Ty_a*_error} and visually in Figures~\ref{fig:FIPO_output_1} and~\ref{fig:FIPO_output_2} that this is not the case.
\begin{equation}
    \left| \underline{y}^\ast - \Phi_y \alpha^\ast \right| = 3.7798
    \label{eq:y*_Ty_a*_error}
\end{equation}
\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full input Partial output - Goal Output 1.pdf}} 
        \caption{}
        \label{fig:FIPO_output_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full input Partial output - Goal Output 2.pdf}} 
        \caption{}
        ~\label{fig:FIPO_output_2}
    \end{subfigure}
    \caption{Deconstructed $\underline{y}^\ast$s vs the goal constructed from $\alpha^\ast$. $\alpha^\ast$ is found by inverting the the $\underline{y}^\ast$ in the space $\Phi_y$. By then attempting to go back, this highlights the inability of $\Phi_y$ to perfectly capture $\underline{y}^\ast$.}
\end{figure}


Now we have a $\Phi_u$ that meets earlier assumptions (input is fully described in its space) and we are correctly violating the assumption that the output is fully defined, we now see how our system performs.

Having defined our goal and our controller -- the first steps for any ILC problem, we can proceed with our test. As before we must conduct at least one trial to generate $e_0$. If we set $\beta_0 = 0_{\eta_u \times 1}$, then $u_0 = 0_{r_{ILC} \times 1}$ and $\underline{y}_0 = \underline{d}$. Just as before, we can compute $e_0 = \underline{y}^\ast - \underline{y}_0$ -- but now we must convert into the output basis space $\Phi_y$ before we can proceed onto the next trial, by $e_{\alpha_0} = \Phi_y^+ e_0$\footnote{It also works to convert the output into the output basis space and compute $e_{\alpha_j} = \alpha^\ast - \alpha_j$}. 

Armed with our first error, we now iteratively apply our controller. Using the previous trials error in the $\eta_y$ space and controller $\mathcal{L}_\beta$, we compute our change in betas $\delta_j \beta$. We use that to compute our new $\beta_{j}$, convert it from the $\Phi_u$ space to get $\underline{u}$. Applying our new sequence of inputs, our system will produce new outputs to calculate an error with and calculate our $e_{\alpha_j}$. Repeat this process as needed.

Through these trials, an amazing property emerges. Even though we cannot capture $\underline{y}^\ast$ perfectly in $\Phi_y$, we are still able to inform our controller with error data that it enables it to send the error to zero. Figures~\ref{fig:FIPO_alpha_error_progression} and~\ref{fig:FIPO_beta_error_progression} show that through trials, the error on both $\alpha$ and $\beta$ are sent to zero.

\begin{figure}[htbp] 
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Partial Output - Alpha Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:FIPO_alpha_error_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Partial Output - Beta Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:FIPO_beta_error_progression}
    \end{subfigure}
    \caption{Progression of Errors on coefficients through perfect-knowledge controller trials when $\underline{u}^\ast \in \Phi_u$ but $\underline{y}^\ast \notin \Phi_y$. Even though $\underline{y}^\ast \notin \Phi_y$, the associated coefficient errors can still go to zero.}
\end{figure}

We can additionally see the progression of the coefficients through trials in 
Figures~\ref{fig:FIPO_alpha_progression} and~\ref{fig:FIPO_beta_progression}. 
We intentionally only plot a few of the coefficients to prevent a cluttered plot.
\begin{figure}[htbp]
    \centering 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Partial Output - Alpha Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:FIPO_alpha_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Partial Output - Beta Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:FIPO_beta_progression}
    \end{subfigure}
    \caption{Progression of Coefficients through perfect-knowledge controller trials when $\underline{u}^\ast \in \Phi_u$ but $\underline{y}^\ast \notin \Phi_y$}
\end{figure}

Figures~\ref{fig:FIPO_mass_1_position} through~\ref{fig:FIPO_shaped_output} show that the zero-error in the basis space translates to zero-error in the full-dimension space.
\begin{figure}[htbp] 
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width = \textwidth]{{Basis Functions/Full input Partial output - Mass 1 Position.pdf}} 
        \caption{}
        \label{fig:FIPO_mass_1_position}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width = \textwidth]{{Basis Functions/Full input Partial output - Mass 2 Position.pdf}} 
        \caption{}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Partial Output - Shaped Output.pdf}} 
        \caption{}
        \label{fig:FIPO_shaped_output}
    \end{subfigure}
    \caption{Progression of outputs through ILC trials when $\underline{u}^\ast \in \Phi_u$ but $\underline{y}^\ast \notin \Phi_y$}
\end{figure}


\FloatBarrier\paragraph{PIFO}
Obviously the next thing to check is what if $\underline{y}^\ast$ is fully defined in $\Phi_y$ but $\underline{u}^\ast$ is not in $\Phi_u$? Or in our terminology, there is a set of \ac{PIFO} describing those spaces. It is incredibly easy to ensure our $\underline{y}^\ast$ is captured because, if you recall our ideal basis space condition, we can simply set one of our basis functions $\phi = \underline{y}^\ast$ at the same time that we set $\underline{y}^\ast$. 

So now our $\underline{u}^\ast$ will only partially be described in $\Phi_u$, but $\underline{y}^\ast$ will be fully in $\Phi_y$.

Ensuring we capture $\underline{u}^\ast$ is understandably much harder without perfect system knowledge. With our current understanding, the only way to guarantee $\underline{u}^\ast$ is in $\Phi_u$ is to expand it to be $r_{ILC} \times r_{ILC}$ and full rank. Obviously this high dimension hurts us, So let us test if it is completely necessary for $\underline{u}^\ast \in \Phi_u$.

We will repeat a similar process as the above example, except now we define $\underline{y}^\ast$ first. For consistency, we will re-use the same parameters as those in Eq.~\ref{eq:beta_star_in_basis}, except now for $\alpha^\ast$
\begin{equation}
    \alpha^\ast = {\begin{bmatrix}1 & 0.2 & -0.3 & 4 & 0 & 0 & 0 & -1 & 0 & 0\end{bmatrix}}^T
\end{equation}
Remembering to split $\underline{y}^\ast$ and plotting the goal outputs separately, we see our goals in Figures~\ref{fig:PIFO_output_1} and~\ref{fig:PIFO_output_2}. It should be no surprise that they look exactly the same as the goal inputs in the previous example; they are defined the exact same.
\begin{figure}[htbp]        
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial input Full output - Goal Output 1.pdf}} 
        \caption{}
        \label{fig:PIFO_output_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial input Full output - Goal Output 2.pdf}} 
        \caption{}
        \label{fig:PIFO_output_2}
    \end{subfigure}
    \caption{Goal Outputs deconstructed from a $\underline{y}^\ast$ explicitly constructed from basis functions to be in $\Phi_y$}
\end{figure}

We can back out our input 
\begin{equation}
    \underline{u}^\ast = P^+ (\underline{y}^\ast - \underline{d})
    \label{eq:u*_from_Py*}
\end{equation}
From there we calculate
\begin{align}
    \beta^\ast &= \Phi_u^+ \underline{u}^\ast  \tag{\ref{eq:b_pinv_Tu_u}} \\
    &= \begin{bmatrix}
        1,555.4  \\  4,385.8 \\   2,871.3 \\   4,453.6   \\ 2,478.4  \\  4,508.5 \\   1,871.0 \\   4,618.8  \\  1,124.2  \\  3,643.1
    \end{bmatrix}
\end{align}
This can be numerically confirmed to not capture our input in Eq.~\ref{eq:u*_Tu_b*_error} and visually in Figures~\ref{fig:PIFO_input_1} and~\ref{fig:PIFO_input_2}
\begin{equation}
    \left| \underline{u}^\ast - \Phi_u \beta^\ast \right| = 1.2514 \times 10^6
    \label{eq:u*_Tu_b*_error}
\end{equation}
\begin{figure}[htbp] 
    \centering 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial input Full output - Goal Input 1.pdf}} 
        \caption{}
        \label{fig:PIFO_input_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering  
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial input Full output - Goal Input 2.pdf}} 
        \caption{}
        \label{fig:PIFO_input_2}
    \end{subfigure}
    \caption{Deconstructed Goal $\underline{u}^\ast$s from $\Phi_u\beta^\ast$, where $\beta^\ast$ was backed out of $\underline{y}^\ast$ with perfect knowledge.}
\end{figure}

Having confirmed we have a $\Phi_u$ that meets earlier assumptions (input is fully described in its space) and we are correctly violating the assumption that the output is fully defined, we now see how our assumptions perform.  

Once again, now that we have our goal and our controller we can proceed to generate $e_0$. Unsurprisingly, we follow the exact same steps as before\footnote{See Appendix~\ref{code:ilc_sim} for a more explicit showing of this}.

Here we begin to see the limits of our basis functions. While figure~\ref{fig:PIFO_alpha_error_progression} shows that we can send $e_\alpha$ to zero, no other parameter does so. Figure~\ref{fig:PIFO_error_progression} shows that through trials, we reach a steady state error. Recall earlier from Eq.~\ref{eq:a_pinv_Ty_y} that multiple $\underline{y}$s can produce the same $\alpha$. We are able to generate $\underline{y}^\ast$, but there is some additional $\underline{y}_{null}$ that exists outside of $\Phi_y$. So we can capture $\underline{y}^\ast$ fully in our basis, but cannot see the error that exists outside of it in the null space. Our input is not producing our $\underline{y}^\ast$, since our $e_\alpha$ goes to zero, our betas stop learning. 
\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Alpha Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:PIFO_alpha_error_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Error Progression.pdf}} 
        \caption{}
        \label{fig:PIFO_error_progression}
    \end{subfigure}
    \caption{Progression of coefficient errors through trials when $\underline{u}^\ast \notin \Phi_u$ but $\underline{y}^\ast \in \Phi_y$. Contrast this with the earlier example where $\underline{u}^\ast \in \Phi_u$, we see now that the error of $\alpha$ can still go to zero, but the error on $\beta$ reaches a non-zero steady state.}
\end{figure}
The progression of the coefficients through trials can be seen Figures~\ref{fig:PIFO_alpha_progression} and~\ref{fig:PIFO_beta_progression}. Once again, only select coefficients are plotted for cleanliness.
\begin{figure}[htbp]
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Alpha Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:PIFO_alpha_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Beta Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:PIFO_beta_progression}
    \end{subfigure}
    \caption{Progression of Coefficients through trials when $\underline{u}^\ast \notin \Phi_u$ but $\underline{y}^\ast \in \Phi_y$}
\end{figure}

Just how drastically off we are can be seen in Figures~\ref{fig:PIFO_mass_1_position} through~\ref{fig:PIFO_shaped_output}
\begin{figure}[htbp] 
    \centering 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Mass 1 Position.pdf}} 
        \caption{}
        \label{fig:PIFO_mass_1_position}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Mass 2 Position.pdf}} 
        \caption{}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Partial Input Full Output - Shaped Output.pdf}} 
        \caption{}
        \label{fig:PIFO_shaped_output}
    \end{subfigure}
    \caption{Progression of Outputs through ILC trials when $\underline{u}^\ast \in \Phi_u$ but $\underline{y}^\ast \notin \Phi_y$}
\end{figure}

\paragraph{FIPO vs PIFO}
Even if the output is not fully captured in a basis space, you may recall that in the ILC problem that is not what we are concerned with. In ILC we deal with the error; more specifically sending the error to zero. No matter our basis space, $e_j = \Phi_y 0_{\eta_y \times 1} = 0_{\ell \times 1}$ due to the zero-matrix identity (just how multiplying by 0 always equals 0). So no matter our goal output, our goal error is always enclosed in the basis space. 

It would be natural then to think the same applies for the input. After all, in a properly converged ILC problem $\delta_j \underline{u}$ should also be all zeros. However as shown in our PIFO example, this hope is disproven. Remember that the ILC problem is trying to `learn' the $\underline{u}^\ast$ that produces $\underline{y}^\ast$ (Eq.~\ref{eq:y*_Pu*_d}), and does this by changing inputs between trials, so while to goal $\delta_j \underline{u}$ may be within $\Phi_u$, we need to be able to get there.

This can be readily seen by falling back to the logic of deadbeat controllers. Imagine a scenario where we have a deadbeat $\mathcal{L}_\beta$. We start with $\beta_0 = 0_{\eta_u \times 1}$, apply it to produce $\underline{y}_0 = \underline{d}$, and compute $e_{\alpha_0} = \Phi_y (\underline{y}^\ast - d)$. 
We follow our control law of Eq.~\ref{eq:basis_ilc_control_law} to compute $\delta_1 \beta = \mathcal{L}_\beta e_{\alpha_0}$, such that $\beta_1 = \beta_0 + \delta_1 \beta$. Given that $\beta_0$ was a zero-vector, we can ignore it, and since we are saying that $\mathcal{L}_\beta$ is deadbeat, we can say that $\beta_1$ is our best attempt at $\beta^\ast$, meaning our best attempt at $\underline{u}^\ast$ must be in $\Phi_u$.

If one were to repeat this example with a non-deadbeat controller, they would find that any $\beta$ found would always produce a $\underline{u}$ in the space $\Phi_u$. So while it is possible to always achieve a $\Phi_y$ that captures our goal of zero error, the space defining the inputs must be selected carefully to include $\underline{u}^\ast$. It is also important to note that the learned input is \textbf{not} the projection of $\underline{u}^\ast$ onto $\Phi_u$\footnote{Unless $\underline{u}^\ast \in \Phi_u$ of course}.

So our $\Phi_u$ matters, but $\Phi_y$ does not.\footnote{This is consistent with the findings of \cite{PhanFrueh1996}}. Given that, why not define $\Phi_y$ such that $\eta_y = 1$. Even if we then leave $\Phi_u$ as the identity matrix, then thinking ahead to our RL problem, we would have the ability to infinitely reduce our `state' dimensions down to $1$. Then input-decoupling could be employed such that each controller could be updated in just $4$ trials, and that would only then need to be done $r_{ILC}$ times. 

\FloatBarrier\paragraph{FISO}
To test this theory, we will setup our problem very much like we did the FIPO example. $\beta^\ast$ will once again be defined as it is in Eq.~\ref{eq:beta_star_in_basis}, with our $\Phi_u$ similarly following Eq.~\ref{eq:Tu_in_cheby}. We will define $\underline{y}^\ast = P\underline{u}^\ast + \underline{d}$. So we have a set of \ac{FISO} describing those spaces.

We have already seen this input sequence in Figures~\ref{fig:FIPO_input_1} and~\ref{fig:FIPO_input_2}, and show the outputs in Figures~\ref{fig:FISO_output_1} and~\ref{fig:FISO_output_2}. 
\begin{figure}[htbp] 
    \centering 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full input Single output - Goal Output 1.pdf}} 
        \caption{}
        \label{fig:FISO_output_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full input Single output - Goal Output 2.pdf}} 
        \caption{}
        \label{fig:FISO_output_2}
    \end{subfigure}
    \caption{Deconstructed Goal Outputs for $\underline{u}^\ast \in \Phi_u$ and $\underline{y}^\ast = \Phi_y$}
\end{figure}
However, the learning history is quite different. Even though we appear to meet all the criteria of capturing our signals in our basis space, we are unable to send the error to zero. It may look ok, but the average error on the output of each point\footnote{Computed as $\frac{\left|e\right|}{n_{ILC}}$} is 0.9823
\begin{figure}[htbp] 
    \centering 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Alpha Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:FISO_alpha_error_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Beta Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:FISO_beta_error_progression}
    \end{subfigure}
    \caption{Progression of coefficient errors through trials when $\underline{u}^\ast \in \Phi_u$ and $\underline{y}^\ast = \Phi_y$}
\end{figure}


\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Alpha Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:FISO_alpha_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering  
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Beta Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:FISO_beta_progression}
    \end{subfigure}
    \caption{Progression of coefficients through trials when $\underline{u}^\ast \in \Phi_u$ and $\underline{y}^\ast = \Phi_y$}
\end{figure}

\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Mass 1 Position.pdf}} 
        \caption{}
        \label{fig:FISO_mass_1_position}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Mass 2 Position.pdf}} 
        \caption{}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Full Input Single Output - Shaped Output.pdf}} 
        \caption{}
        \label{fig:FISO_shaped_output}
    \end{subfigure}
    \caption{Progression of Outputs through ILC trials when $\underline{u}^\ast \in \Phi_u$ and $\underline{y}^\ast = \Phi_y$. Even though $\underline{u}^\ast$ could in theory be fully described by $\Phi_u$, the controller is unable to capture $\underline{y}^\ast$}
\end{figure}
We see that we are still able to send $e_\alpha \to 0$, or $\alpha \to 1$, but the other components do not follow. Once again we are faced with this steady-state error on the beta.

\FloatBarrier\paragraph{SIFO}
We now check the other scenario, where the spaces can be described with a set of \ac{SIFO}. This is an extreme version of our POFI example, so we set it up the same. The difference being we extract $\underline{u}^\ast$ in Eq.~\ref{eq:u*_from_Py*}, we also use that to set our input basis space such that $\underline{u}^\ast = \Phi_u$. The goals can be seen in Figures~\ref{fig:PIFO_output_1} and~\ref{fig:PIFO_output_2}, and the inputs that get us there in Figures~\ref{fig:SIFO_input_1} and~\ref{fig:SIFO_input_2} -- nothing new here.
\begin{figure}[htbp] 
    \centering 
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single input Full output - Goal Input 1.pdf}} 
        \caption{}
        \label{fig:SIFO_input_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}  
        \includegraphics[width=1\textwidth]{{Basis Functions/Single input Full output - Goal Input 2.pdf}} 
        \caption{}
        \label{fig:SIFO_input_2}
    \end{subfigure}
    \caption{Deconstructed Goal Inputs for $\underline{u}^\ast = \Phi_u$ and $\underline{y}^\ast \in \Phi_y$}
\end{figure}

What is new, however, is the learning process. We see we are able to actually find our output, with the average error on each point being $8.85 \times 10^{-5}$ - within our `zero' range.
\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Alpha Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:SIFO_alpha_error_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Beta Error Norm Progression.pdf}} 
        \caption{}
        \label{fig:SIFO_beta_error_progression}
    \end{subfigure}
    \caption{Progression of coefficient errors through trials when $\underline{u}^\ast = \Phi_u$ and $\underline{y}^\ast \in \Phi_y$}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Alpha Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:SIFO_alpha_progression}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth} 
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Beta Coeffecient Progression.pdf}} 
        \caption{}
        \label{fig:SIFO_beta_progression}
    \end{subfigure}
    \caption{Progression of coefficients through trials when $\underline{u}^\ast = \Phi_u$ and $\underline{y}^\ast \in \Phi_y$}
\end{figure}

\begin{figure}[htbp] 
    \centering  
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Mass 1 Position.pdf}} 
        \caption{}
        \label{fig:SIFO_mass_1_position}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Mass 2 Position.pdf}} 
        \caption{}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.8\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Single Input Full Output - Shaped Output.pdf}} 
        \caption{}
        \label{fig:SIFO_shaped_output}
    \end{subfigure}
    \caption{Progression of Positions through ILC trials when $\underline{u}^\ast = \Phi_u$ and $\underline{y}^\ast \in \Phi_y$. While the learned shape may appear arbitrary, the important take-away is that the error is zero, and the learned output matches that exactly of the goal.}
\end{figure}

\FloatBarrier\paragraph{FISO vs SIFO}
What this shows us is that in addition to our conditions on $\Phi_u$, and lack thereof on $\Phi_y$, we must meet certain criteria when selecting our dimensions. It can be shown that condition is $\eta_y \geq \eta_u$.

If we refer back to our $\eta_u \times \eta_y$ controller in Eq.~\ref{eq:basis_ilc_control_law}, this makes sense.\footnote{The wording here will get a little messy, as the input to a controller is the output of a system, and the output of a controller is the input to the system, so pay close attention to that.} For robust control, we cannot ask a controller to produce more outputs than inputs if we want those controller outputs to be the best system inputs. 

\FloatBarrier\subsubsection{Summary}
We have established two conditions and one freedom. First, given that a $\underline{u}^\ast$ exists, our input basis space $\Phi_u$ must include it.
\begin{equation}
    \underline{u}^\ast \in \text{span($\Phi_u$)}
    \label{eq:basis_ilc_u_star_condition}
\end{equation}
Our second condition is we must have as many, if not more, basis functions describing the output than we do the input of the system.
\begin{equation}
    \eta_y \geq \eta_u
    \label{eq:basis_ilc_num_basis_condition}
\end{equation}
The only afforded freedom is that $\Phi_y$ does not need to be chosen wisely, and will always allow for a controller to send the error to zero.

\FloatBarrier\subsection{Creating a Dynamic Basis Space from Learned Inputs}
~\label{sub:dynamic_arb_basis}
To effectively learn, we are limited by our ability to capture $\underline{u}^\ast$. For every reduction we can make to $\eta_u$, we can similarly reduce $\eta_y$. The idea is to then determine the best possible input for a set of basis functions, then change our basis functions to include that learned input. 

For a signal of length $\ell$, we will at most need $\ell$ basis functions. To start, we define a complete basis space to draw from that we know will capture the entire input. As we now know that the distinction between $\Phi_y$ and $\Phi_u$ does not matter, we will now use the same notation of $\Phi$ for both\footnote{For our given examples we have a 2-input, 2-output system such that $n_{ILC} = r_{ILC}$ but this is not always the case. In that case, $\Phi_y$ still does not matter, but may need to be defined in different dimensions and thus not be able to equal $\Phi_u$.}.
\begin{equation}
    \Phi_{full} =
    \begin{bmatrix}
        | & | &  & | \\
        \phi_1 & \phi_2 & \cdots & \phi_{\ell} \\
        | & | &  & | \\
    \end{bmatrix}
    \label{eq:full_basis_space_phi}
\end{equation}

Next we define our $\eta$s. As per the condition stated in Eq.~\ref{eq:basis_ilc_num_basis_condition}, we will set $\eta_y = \eta_u$ and use $\eta$ for both now. This is both out of convenience and efficiency.

This is all we need to begin our process. To set some notation and terminology, we will refer to each attempt with a new basis space as an `episode'. Each episode will have its own $\ell \times \eta$ basis space $\Phi^e$, and will learn an input $\underline{u}^e$\footnote{This is the input which generates $e_\alpha = 0$.}.

After each episode, we take the learned input $\underline{u}^e$ and set that as a basis function. We then update the remaining $\eta - 1$ basis functions with new $\phi$s from $\Phi_{full}$, as shown in Eq~\ref{eq:rolling_basis}.
\begin{equation}
    \Phi^{e+1} =
    \begin{bmatrix}
        | & | &  & | \\
        \underline{u}^e & \phi_j & \cdots & \phi_{j+\eta-1} \\
        | & | &  & | \\
    \end{bmatrix}
    \label{eq:rolling_basis}
\end{equation}
where $1 \leq j \leq \ell$, marking what basis function we draw from $\Phi_{full}$, and `rolls-over' as needed. So if $p=\ell=10$, a $j$-sequence would be $j = 1, 2, \dots 10, 1, \dots$.

One would hope that after sufficient episodes, there would be a $\Phi^e$ found such that $\underline{u}^\ast$ is included.

\FloatBarrier\subsubsection{Example -- Rolling Basis Space}
First we set up the problem as done in the FIPO example, with $\underline{u}^\ast$ shown in Figures~\ref{fig:FIPO_input_1} and~\ref{fig:FIPO_input_2} and $\underline{y}^\ast$ in Figures~\ref{fig:FISO_output_1} and~\ref{fig:FISO_output_2}.

Although we have a signal of length $\ell = 200$\footnote{$n_{ILC} = r_{ILC} = 200$}, we do not need $\ell$ basis functions to ensure we capture our $\underline{u}^\ast$. Recall for our FIPO example, we defined $\underline{u}^\ast$ in the space of Chebyshev Polynomials $T_0 \to T_9$, so we can define $\Phi_{full}$ as we define $\Phi_u$ in Eq.~\ref{eq:Tu_in_cheby} to ensure we capture $\underline{u}^\ast$. If we did not know this to be the case, we would have to define $\Phi_{full}$ to be a full rank $200 \times 200$ matrix to ensure we spanned the whole input space.
\begin{equation}
    \Phi_{full} =
    \begin{bmatrix}
        T_0 & T_1 & \cdots & T_9
    \end{bmatrix}
\end{equation}

We next set our $\eta$, and for this example $\eta = 4$. For our first episode we do not have a previously learned input, so we will use a basis function. 
\begin{equation}
    \Phi^0 =
    \begin{bmatrix}
        T_0 & T_1 & T_2 & T_3
    \end{bmatrix}
\end{equation}

Recall that our $\underline{u}^\ast$ is defined with weights on $T_0 \to T_3$, but also $T_7$. If the learned input for a given basis space was the projection of $\underline{u}^\ast$ onto that basis space, we would expect $\beta_{hoped}^e = {\begin{bmatrix}1 & 0.2 & -0.3 & 4\end{bmatrix}}^T$. As previously stated, this is not the case and so we end up with $\beta^e = {\begin{bmatrix}1.2596  &  0.7884  &  0.0403  &  4.2582\end{bmatrix}}^T$

The controller found a way to send $e_\alpha$ (as shown in Figure.~\ref{fig:rolling_basis_alpha_error}) just as it was able to in our PIFO example. This does not contradict with any of our earlier observations.

We now construct a new $\Phi$ with the learned input as one of the basis functions
\begin{equation}
    \Phi^1 =
    \begin{bmatrix}
        \underline{u}^0 & T_4 & T_5 & T_6
    \end{bmatrix}
\end{equation}
and repeat the process. Since we are using $\Phi = \Phi_u = \Phi_y$, make sure to update both basis spaces. If $\Phi_y$ is left fixed, although it does not matter, learning will be sub-optimal. This is because our first pass finds the $\underline{u}$ that sends $e_{\alpha_0}$, and so if we do not update $\Phi_y$, then that same learned input will be applied again with the same results - the system will see no added benefit from including new inputs.

We do this enough times to try every every basis function in $\Phi_{full}$ multiple times and see that each time while we are able to send $e_\alpha \to 0$, we never find $\underline{u}^\ast$ nor send $e \to 0$. 

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=0.49\textwidth]{{Basis Functions/Rolling Input Basis - Alpha Error Norm Progression.pdf}} 
    \caption{Progression of $e_\alpha$ through rolling basis space episodes}
~\label{fig:rolling_basis_alpha_error}
\end{figure}

It can be seen that this approach does come close to producing our goal output in Figure~\ref{fig:rolling_basis_shaped}, and can be tempting to say it works. While it may be acceptable in some processes, it is not the exact model we are seeking. This is most evident by inspecting the learned inputs in Figures~\ref{fig:rolling_basis_input1} and~\ref{fig:rolling_basis_input2}, and seeing they do not match $\underline{u}^\ast$. Additionally, we have a final error magnitude $\left|e\right| = 0.0132$\footnote{Recall that Matlab `zero' $\leq 1\times10^{-6}$}.

\begin{figure}[htbp] 
    \centering  
    \includegraphics[width=0.8\textwidth]{{Basis Functions/Rolling Input Basis - Error Progression.pdf}} 
    \caption{Progression of $e$ through rolling basis space episodes}
    \label{fig:rolling_basis_error}
\end{figure}


\begin{figure}[htbp] 
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Rolling Input Basis - Input 1.pdf}} 
        \caption{}
        \label{fig:rolling_basis_input1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Rolling Input Basis - Input 2.pdf}} 
        \caption{}
        \label{fig:rolling_basis_input2}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{{Basis Functions/Rolling Input Basis - Shaped Output.pdf}} 
        \caption{}
        \label{fig:rolling_basis_shaped}
    \end{subfigure}
    \caption{Progression of deconstructed goal inputs and the generated shaped output through rolling basis space episodes. While the output can be seen to be close to our goal, even after working our way through the entirety of the input basis space, we still fail to properly capture our $\underline{y}^\ast$. Failure to capture $\underline{u}^\ast$ can best be seen on the fringes of the input signals.}
\end{figure}

\begin{figure}[htbp] 
    \centering
    \includegraphics[width=0.8\textwidth]{{Basis Functions/Rolling Input Basis - Beta Coeffecient Progression.pdf}} 
    \caption{Progression of $\beta$ through rolling basis space episodes. After sufficient trials have been made to cover all of $\Phi_{full}$, it can be seen that $\beta_1$ sits around a value of 1 - meaning that the learned input is predominately that of the previously learned pass. However, there are still tiny additions from other components, evident by the non-zero $\beta_{2-4}$.}
    \label{fig:rolling_basis_betas}
\end{figure}

Figure~\ref{fig:rolling_basis_error} shows that after our first pass through all of the input basis we have a `petty good' input. This is logical, as it has now taken into consideration every possible basis function that makes up our $\underline{u}^\ast$. However, Figure~\ref{fig:rolling_basis_betas} highlights the fact that while after the controller has been exposed to all the necessary information to span the space, it still attempts to make slight modifications to the learned input of the previous trial -- it fails to find the $\underline{u}^\ast$. 

In Figure~\ref{fig:rolling_basis_betas_forced} we `force' the learned input that goes on to make up the next $\Phi$ to be exactly $\underline{u}^\ast$. It can be seen that the system, if it finds the proper $\underline{u}^\ast$ in its cycle will properly stay converged on that solution, so the prior example clearly did not find our goal input.

\begin{figure}[htbp] 
    \centering
    \includegraphics[width=0.8\textwidth]{{Basis Functions/Forced Rolling Input Basis - Beta Coeffecient Progression.pdf}} 
    \caption{Progression of $\beta$ through rolling basis space episodes. After 7 iterations, $\Phi$ is forced to have $\underline{u}^\ast$ in it. It is demonstrated that the controller the continues to properly identify that as the correct input. It can be seen, given this, that the earlier example never actually finds $\underline{u}^\ast$ before being moved away -- once $\underline{u}^\ast$ is found, it is not lost.}
    \label{fig:rolling_basis_betas_forced}
\end{figure}

\FloatBarrier\subsubsection{Summary of Dyanmic Spaces}
While it is possible to start with a finite number, $\eta$, of basis functions $\phi$ to create a dynamic basis space which better captures $\underline{u}$, there is no guarantee to create one that captures $\underline{u}^\ast$. 

There are two main reasons for the failings of this approach. Recall that there are numerous $\underline{y}$s that can produce the same same $\alpha$ for a given $\Phi$. Additionally, while our basis functions may be independent of one another, as stated in Eq.~\ref{eq:orthogonal_basis}, they are not necessarily independent in the response they illicit from the system in the basis space. So in addition to the different $\underline{y}$s that can get us $\alpha^\ast$, there are also multiple $\underline{u}$s to get us to $\alpha^\ast$. Since different inputs can lead to the same `perceived' output through the lens of a basis space, defining our basis space and functions as such do not ensure that we are able to find the optimal $\underline{u}^\ast$

\FloatBarrier\subsection{Summary of Basis Function}
Basis Functions offer some path forward for dimension reduction, but currently it is not an exact one. We have shown that the number of basis functions on the system must adhere by $\eta_u \leq \eta_u$, and that our input basis space $\Phi_u$ must capture $\underline{u}^\ast$ -- no such condition exists for $\underline{y}^\ast$ and $\Phi_y$. We have additionally shown that while it is possible to work with a dynamic basis space $\Phi$, the possibility that different inputs can create the same $\alpha$ -- or projection of $\underline{y}$ on $\Phi_y$ -- makes exact control impossible.