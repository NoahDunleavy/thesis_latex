%Warnings to suppress (manually decide for each file)
% chktex-file 8 %suppress warning about dash lengths
\section{Reinforcement Learning on Iterative Learning Control}
~\label{sec:rl_on_ilc}
Recall our earlier definition of the ILC system:
\begin{equation}
	e_{j+1}=Ie_j-P\delta_{j+1}\underline{u}
    \tag{\ref{eq:ILC_law}}
\end{equation}
And its controller
\begin{equation}
	\delta_{j+1}\underline{u}=\mathcal{L}e_j
    \tag{\ref{eq:del_u_L_e_j}}
\end{equation}
It is logical to draw parallels from this formulation to that our ABCD formulation
\begin{equation}
    x\left(k\ +\ 1\right)=Ax\left(k\right)+Bu\left(k\right)
    \tag{\ref{eq:discrete_state_space_model}}
\end{equation}
\begin{equation}
    y\left(k\right)=Cx\left(k\right)+Du\left(k\right)
    \tag{\ref{eq:discrete_state_space_output}}
\end{equation}
Where our `state' is now the error $e_j$ and our `input' is the change in inputs $\delta_{j+1}\underline{u}$. We will refer to the error term as our `ILC State' and the change in inputs as our `ILC Input'. 

As previously shown, our `ILC State' ($n_{ILC}$) is now a $pm \times1$ vector, and our `ILC Input' ($r_{ILC}$) is $pr \times1$. However, a state is still and state, and an input is an input. So the principals of RL still apply to find a controller, now demarked $\mathcal{L}$, that sends our error state to zero. Thus our utility function that defines the cost function that the found controller will minimize is
\begin{equation}
    U_j = {\delta_j \underline{u}}^T R {\delta_j \underline{u}} + {e_j}^T Q {e_j}
    \label{eq:ilc_utility}
\end{equation}

\FloatBarrier\subsection{Example --- Policy Iteration on ILC}
We first begin by setting the number of steps in our process. We will work with a $p=10$ (why not the same 100 we used earlier will be shown shortly). Given our two-input, two-output system, that means we have 20 effective states and 20 effective inputs. We use this resolution to then set our goal output. We will replicate the earlier goals of
\begin{equation}
    y_1^\ast=\cos\left(\frac{2\pi k}{p}\right)
    \quad
    y_2^\ast=\sin\left(\frac{2\pi k}{p}\right)
    \tag{\ref{eq:y1_y2_star}}
\end{equation}
stacked as in Eq.~\ref{eq:stacked_y_star}

Next, as we are operating without a controller, we must define our cost matrices
\begin{equation}
    Q=100\cdot I_{20\times 20}
    \quad
    R=1\cdot I_{20\times 20}
    \quad
    \gamma=\ 0.8
    \label{eq:rl_ilc_params}
\end{equation}
Using the complete knowledge afforded to us in simulation, we can find the $20 \times20$ controller $\mathcal{L}_{LQR}^\gamma$
\begin{equation}
    \mathcal{L}_{LQR}^\gamma=\left[\begin{matrix}0.0198&-0.0000&\cdots&0.2288&0.1583\\0.0001&0.0393&\cdots&0.1583&0.3778\\\vdots&\vdots&&\vdots&\vdots\\-0.0000&-0.0000&\cdots&0.0198&0.0001\\-0.0000&-0.0000&\cdots&-0.0000&0.0393\\\end{matrix}\right]
    \label{eq:ilc_lqr_controller}
\end{equation}
Given our ILC state-input dimensions, $X_j\left(k\right)$ will be $1 \times1,600$ $({\left(n+r\right)}^2={\left(20+20\right)}^2)$. This does not even mean 16 seconds from our systems $\Delta t\ = 0.01$ seconds, but it is 1,600 trials of 10 steps ($p$), which means 160 seconds (ideally) and lots of potentially wasted parts for a single controller update. In this example, we will learn five controllers.

We start with a controller of all zeros, and begin learning just as before. There must be one initial trial outside of the learning to generate our first `state' of error. It is logical to have this be the open loop behavior, in which case the output $\underline{y}$ is equal to the noise and initial conditions parameter $\underline{d}$ (from Eq.~\ref{eq:y_Pu_d}). For each trial, we compute the change in inputs,
\begin{equation}
    \delta_j\underline{u}=\mathcal{L}e_{j-1}+v_j
    \label{eq:rl_ilc_input}
\end{equation}
where $v_j$ is our exploration term. In this case, $v_j$ is normally distributed around 0 and covers ranges [-1, 1]. We then compute and apply the system input as ${\underline{u}}_j={\underline{u}}_{j-1}+\delta_j\underline{u}$. Keep in mind the distinction between the input to the system, ${\underline{u}}_j$, and the input to the ILC learning of $\delta_j\underline{u}$.

Upon applying ${\underline{u}}_j$, our system will produce a sequence of outputs. Recall that as we cannot control $y\left(0\right)$, we exclude it from our model such that
\begin{equation}
    \underline{y}=\left[\begin{matrix}y\left(1\right)\\\vdots\\y\left(p\right)\\\end{matrix}\right]
\end{equation}
Computing $e_j$ from ${\underline{y}}^\ast-{\underline{y}}_j$, we have all the information we need to proceed with learning and the next ILC trial.

For the purposes of learning, observe the following analogies, where we translate the RL formatting to one in-line with the ILC system
\begin{align}
    x\left(k\right)&\rightarrow e_{j-1}  \\
    u\left(k\right)&\rightarrow\delta_j\underline{u} \\
    x\left(k+1\right)&\rightarrow e_j    \\
    Fx\left(k\right)&\rightarrow\mathcal{L}e_j   \\
\end{align}
So our $1 \times1,600 \ X_j$ is 
\begin{equation}
    X_j={\left[\begin{matrix}e_{j-1}\\\delta_j\underline{u}\\\end{matrix}\right]}^T\otimes{\left[\begin{matrix}e_{j-1}\\\delta_j\underline{u}\\\end{matrix}\right]}^T-{\gamma\left[\begin{matrix}e_j\\\mathcal{L}e_j\\\end{matrix}\right]}^T\otimes{\left[\begin{matrix}e_j\\\mathcal{L}e_j\\\end{matrix}\right]}^T
\end{equation}
We repeat the ILC process until we have enough trials to solve for our $\textbf{P}_j^S$, unstack it, impose symmetry, and update our controller as
\begin{equation}
    \mathcal{L}=-{\left(\textbf{P}_{uu}\right)}^{-1}\left(\textbf{P}_{xu}^T\right)
\end{equation}
Where $\textbf{P}_{uu}$ is now the bottom-right $20\times20$ matrix in $P_j$, and $\textbf{P}_{xu}^T$ the bottom-left $20\times20$. For the listed example, we repeat this process 4 more times (for a total of 5 controllers). Our end controller is
\begin{equation}
    \mathcal{L}_{policy}=\left[\begin{matrix}0.0198&-0.0000&\cdots&0.2288&0.1583\\0.0001&0.0393&\cdots&0.1583&0.3778\\\vdots&\vdots&&\vdots&\vdots\\-0.0000&-0.0000&\cdots&0.0198&0.0001\\-0.0000&-0.0000&\cdots&-0.0000&0.0393\\\end{matrix}\right]
    \label{eq:rl_ilc_policy_controller}
\end{equation}
With an error magnitude of $8.22\times {10}^{-11}$, computed as
\begin{equation}
    \frac{\left|\mathcal{L}_{policy}-\mathcal{L}_{LQR}^\gamma\right|}{p^2mr}
    \label{eq:controller_error_calc}
\end{equation}
Where we use the `norm' operator in the numerator to compute the absolute magnitude of the differences, then scale down by the number of elements in each controller. Computationally this result can be treated as zero. So via RL we can still extract the exact LQR controller. The controller learning process can be seen in Figures~\ref{fig:policy_ilc_F1}-~\ref{fig:policy_ilc_F20}.

Each input number refers to the step along the process in which it is being applied, and the state refers to the errors of the previous trial. 

Note that we only are showing a few select weights and inputs, as each of the twenty inputs are informed by each of the twenty errors, which would make for a very cluttered set of plots.

\begin{figure}[htbp] %controller histories
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration ILC Controller - Input 1 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 1 through Policy Iteration Trials}%
        \label{fig:policy_ilc_F1}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration ILC Controller - Input 7 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 7 through Policy Iteration Trials}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration ILC Controller - Input 13 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 13 through Policy Iteration Trials}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration ILC Controller - Input 20 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 20 through Policy Iteration Trials}%
        \label{fig:policy_ilc_F20}
    \end{minipage}
\end{figure}

\begin{figure}[htbp] %error progression
    \centering  %center align
    \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration on ILC - Error Progression.pdf}}
    \caption{Error Magnitude of Output through Policy Iteration Trials}
\end{figure}

\begin{figure}[htbp] %IO history
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration on ILC - Mass 1 Position.pdf}}
        \caption{Mass 1 Positions through Policy Iteration Iterations}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration on ILC - Mass 2 Position.pdf}}
        \caption{Mass 2 Positions through Policy Iteration Iterations}
    \end{minipage}
    \vfill
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration on ILC - Input 1.pdf}}
        \caption{Input 1 Magnitudes through Policy Iteration Iterations}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration on ILC - Input 2.pdf}}
        \caption{Input 2 Magnitudes through Policy Iteration Iterations}
    \end{minipage}
\end{figure}

\begin{figure}[htbp] %shaped
    \centering  %center align
    \includegraphics[width=\textwidth]{{RL on ILC/Policy Iteration on ILC - Shaped Output.pdf}}
    \caption{Shaped Output through Policy Iteration Trials}
\end{figure}

\FloatBarrier~It is clear here that even over our nearly 10,000 trials, we did not perfectly capture our output. This is since our R weighting was relatively large. Further reductions of $R$ does not remove the existence of an LQR controller but introduces issues when computing the pseudo-inverse of $X_j$. 
It is a requirement for $X_j$ to be well-conditioned and composed of sufficient linearly independent equations (full rank). When learning in ILC with a small $R$, we approach a point where both $e_j$ and $\delta_{j+1}u$ are near-zero and we lose the ability to even generate linearly-independent $X_j$s, if we are learning with sequential trials. This is because we tell the system it is ok to make big changes between trials, and in doing so it too-quickly sends our error $e_j$ to zero, meaning it no longer needs to generate a change in inputs $\delta_j \underline{u}$ -- our $X_j$ is then essentially all zeros.

If one wishes to reduce $R$, there are multiple approaches. The first is to collect more trials per input controller. This will obviously increase the total number of trials needed to learn, but by by increasing our collections, we improve our odds of having enough linearly-independent samples. The next approach would be to completely randomize both `state' and `input', to ensure truly arbitrary $e_{j-1}$, $\delta_j\underline{u}$, $e_j$ triplets. This is easy in simulation, but impractical in reality where one likely wishes to learn as they are working through sequential steps. The final option, and the one shown below is to increase the magnitude of your exploration term.
Whereas previous $v_j$ was normally distributed around 0, ranging from [-1, 1], we will now explore in the range of [-1000, 1000]. Keeping the same goal as defined in Eq.~\ref{eq:y1_y2_star}, but redefining our cost-matrices as
\begin{equation}
    Q=100\cdot I_{20 \times 20}
    \quad
    R={10}^{-6}\cdot I_{20 \times 20}
    \quad
    \gamma=\ 0.8
\end{equation}
We have the new LQR Controller
\begin{equation}
    \mathcal{L}_{LQR}^\gamma=\left[\begin{matrix}2,095.3&-44.6&\cdots&1.8&-0.5\\-22.4&2,050.8&\cdots&-0.5&1.4\\\vdots&\vdots&&\vdots&\vdots\\-0.9&0.1&\cdots&2,095.3&-22.4\\0.1&-6.7&\cdots&-44.6&2,050.8\\\end{matrix}\right]
    \label{eq:rl_ilc_lqr_small_R_controller}
\end{equation}
Repeating our Policy Iteration learning process with our new $R$ and $v_j$ amplitude, we see we can once again extract the LQR controller
\begin{equation}
    \mathcal{L}_{LQR}^\gamma=\left[\begin{matrix}2,095.3&-44.6&\cdots&1.8&-0.5\\-22.4&2,050.8&\cdots&-0.5&1.4\\\vdots&\vdots&&\vdots&\vdots\\-0.9&0.1&\cdots&2,095.3&-22.4\\0.1&-6.7&\cdots&-44.6&2,050.8\\\end{matrix}\right]
    \label{eq:rl_ilc_policy_small_R_controller}
\end{equation}
With an error magnitude of $8.55\times{10}^{-6}$, computed as in Eq.~\ref{eq:controller_error_calc}. We see a much sharper error progression in Figure~\ref{fig:reduced_R_ilc_error} that is to be expected imposing such a small cost on the change of inputs. We see this occur right after the first learned controller is applied, which we defined with a small $R$, meaning it had very little penalty to make a large change in inputs - resulting in a sudden drop in error. The progression of controller weights, system outputs, and inputs can also be shown to have sharp drop upon application of the first learned controller. Due to the reduced $R$, we only run 100 trials without the exploration term, and it only takes less than a handful of those for the error to drop to zero.

\begin{figure}[htbp] %controller histories
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration ILC Controller - Input 1 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 1 through Policy Iteration Trials with Reduced R}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration ILC Controller - Input 7 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 7 through Policy Iteration Trials with Reduced R}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration ILC Controller - Input 13 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 13 through Policy Iteration Trials with Reduced R}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration ILC Controller - Input 20 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 20 through Policy Iteration Trials with Reduced R}
    \end{minipage}
\end{figure}

\begin{figure}[htbp] %error progression
    \centering  %center align
    \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration on ILC - Error Progression.pdf}}
    \caption{Error Magnitude of Output through Policy Iteration Trials with Reduced R}%
    \label{fig:reduced_R_ilc_error}
\end{figure}

\begin{figure}[htbp] %IO history
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration on ILC - Mass 1 Position.pdf}}
        \caption{Mass 1 Positions through Policy Iteration Trials with Reduced R}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration on ILC - Mass 2 Position.pdf}}
        \caption{Mass 2 Positions through Policy Iteration Trials with Reduced R}
    \end{minipage}
    \vfill
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration on ILC - Input 1.pdf}}
        \caption{Input 1 Magnitudes through Policy Iteration Trials with Reduced R}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration on ILC - Input 2.pdf}}
        \caption{Input 2 Magnitudes through Policy Iteration Trials with Reduced R}
    \end{minipage}
\end{figure}

\begin{figure}[htbp] %shaped
    \centering  %center align
    \includegraphics[width=\textwidth]{{RL on ILC/Small R Policy Iteration on ILC - Shaped Output.pdf}}
    \caption{Shaped Output through Policy Iteration Trials with Reduced R}%
    \label{fig:shaped_rl_small_R}
\end{figure}

\FloatBarrier~The trade-offs of this approach are obvious. With such a large exploration term, while we are learning we produce no outputs like our goal, as shown in~\ref{fig:shaped_rl_small_R}. Additionally our controller progression is a lot jumpier - displaying characteristics of overshoot. Yet once we are done learning, we can apply this controller and within a very limited number of trials have zero error.
As mentioned at the beginning of the example, this is a relatively low-resolution ILC problem, with $p=10$. What if we were to increase this to $p=100$ as with our basic ILC introduction problem?
Our number of ILC states would climb from 20 to 200, as would our ILC inputs. Which would mean the dimensions of our $X_j$ would go from $1\times1,600$ to $1\times160,000$. $\textbf{P}_j$ just went from having $2.56\times{10}^6$ elements - already a lot - to $2.56\times{10}^{10}$. Monetarily and computationally this makes the RL approach very costly. In fact, if we set $p=100$, MATLAB will not even begin to try and solve the problem. When we go to pre-allocate the array that holds all the stacks of $X_j$s, we are met with the following error:
\begin{center}
    \begin{verbatim}
    Error using zeros
    Requested 160000x160000 (190.7GB) array exceeds maximum 
    array size preference (31.6GB). This might cause MATLAB 
    to become unresponsive.
    \end{verbatim}
\end{center}
so if we want to learn even the most common of ILC problems, we have to somehow reduce our dimensions.

\FloatBarrier\subsection{Example --- Input Decoupling on ILC}
We have already shown that Input Decoupling can be used to reduce the dimensions of $X_j$ from $1\times\left(n+r\right)$ to $1\times\left(n+1\right)$, so it is a logical place to turn to when considering the costs of learning. In our ILC context, it will now take only 441 trials to update a single controller (though this must be done 20 times for a total of 8,820 trials for a complete attempt at $\mathcal{L}_{LQR}^\gamma$).

However while it will take iterating through all $r$ inputs to arrive at the LQR controller, individual controller that we learn will help control the system regardless of whether it is yet our complete LQR.\@ Repeating the parameters and goal from earlier example of Policy Iteration (Eqs.~\ref{eq:y1_y2_star} and~\ref{eq:rl_ilc_params}), we predictably find the same $20 \times20$ controller $\mathcal{L}_{LQR}^\gamma$ (see Eq.~\ref{eq:ilc_lqr_controller}).

The learning process follows just as is the policy iteration example. The only difference being that we are only learning only one controller step at a time, and following input decoupling logic. So when we compute $\delta_j\underline{u}=\mathcal{L}e_{j-1}$, we modify only one of the terms with our exploration, such that
\begin{equation}
    \delta_j\underline{u_i}=\mathcal{L}_i e_{j-1}+v_i
\end{equation}
Now our ILC to Input Decoupling analogies are
\begin{align}
    x\left(k\right)&\rightarrow e_{j-1}
    u_i\left(k\right)&\rightarrow\delta_j\underline{u_i}
    x\left(k+1\right)&\rightarrow e_j
    F_i x\left(k\right)&\rightarrow\mathcal{L}_i e_j
\end{align}
So our $1 \times441$ $X_j$ is 
\begin{equation}
    X_j={\left[\begin{matrix}e_{j-1}\\\delta_j\underline{u_i}\\\end{matrix}\right]}^T\otimes{\left[\begin{matrix}e_{j-1}\\\delta_j\underline{u_i}\\\end{matrix}\right]}^T-{\gamma\left[\begin{matrix}e_j\\\mathcal{L}_i e_j\\\end{matrix}\right]}^T\otimes{\left[\begin{matrix}e_j\\\mathcal{L}_i e_j\\\end{matrix}\right]}^T
\end{equation}
We repeat the ILC process until we have enough trials to solve for our $\textbf{P}_j^S$, unstack it, impose symmetry, and update our controller as
\begin{equation}
    \mathcal{L}_i=-{\left(\textbf{P}_{uu}\right)}^{-1}\left(\textbf{P}_{xu}^T\right)
\end{equation}
Where $\textbf{P}_{uu}$ is now the bottom-right scalar in $P_j$, and $\textbf{P}_{xu}^T$ the bottom-left $20\times20$. For the listed example, we repeat this process 19 more times to cover all the `inputs', then that process 4 more times to generate 5 `complete' controllers. Our end controller is
\begin{equation}
    \mathcal{L}_{decoupled}=\left[\begin{matrix}0.0199&-0.0001&\cdots&0.2312&0.1620\\0.0001&0.0392&\cdots&0.1613&0.3887\\\vdots&\vdots&&\vdots&\vdots\\-0.0003&-0.0000&\cdots&0.0198&-0.0001\\-0.0003&-0.0003&\cdots&-0.0007&0.0396\\\end{matrix}\right]
    \label{eq:decoupled_ilc_controller}
\end{equation}
Which has an error of $1.15\times{10}^{-4}$, as computed in Eq.~\ref{eq:controller_error_calc}. We show the learning process below, once again only showing a few select weights and inputs of the controller.

\begin{figure}[htbp] %controller histories
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling ILC Controller - Input 1 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 1 through Input Decoupling Trials}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling ILC Controller - Input 7 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 7 through Input Decoupling Trials}
    \end{minipage}
    \vfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling ILC Controller - Input 13 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 13 through Input Decoupling Trials}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling ILC Controller - Input 20 Controller Weights.pdf}}
        \caption{Select Controller Weights on Input 20 through Input Decoupling Trials}
    \end{minipage}
\end{figure}

\begin{figure}[htbp] %error progression
    \centering  %center align
    \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling on ILC - Error Progression.pdf}}
    \caption{Error Magnitude of Output through Input Decoupling Trials}
\end{figure}

\begin{figure}[htbp] %IO history
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling on ILC - Mass 1 Position.pdf}}
        \caption{Mass 1 Positions through Input Decoupling Trials}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling on ILC - Mass 2 Position.pdf}}
        \caption{Mass 2 Positions through Input Decoupling Trials}
    \end{minipage}
    \vfill
    \centering  %center align
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling on ILC - Input 1.pdf}}
        \caption{Input 1 Magnitudes through Input Decoupling Trials}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling on ILC - Input 2.pdf}}
        \caption{Input 2 Magnitudes through Input Decoupling Trials}
    \end{minipage}
\end{figure}

\begin{figure}[htbp] %shaped
    \centering  %center align
    \includegraphics[width=\textwidth]{{RL on ILC/Input Decoupling on ILC - Shaped Output.pdf}}
    \caption{Shaped Output through Input Decoupling Trials}
\end{figure}

\FloatBarrier~The input decoupling approach ends up suffering from the same issues as the policy iteration approach, and then some. We could solve the same R issue the same way (more exploration), but we face the issue of rotating controllers. 

While each controller update takes less collections than the Policy approach, it takes 20 controllers to arrive upon a complete controller like the one Policy produces. To generate 5 `complete' controllers (comprised of fresh controllers for all 20 inputs) it takes 44,100 trials via Input Decoupling versus  Policy Iteration's 8,000.

\FloatBarrier\subsection{Summary of RL on ILC}
We have shown that the RL can be used to address the ILC problem. However the dimensions pose a significant limitation on complexity and resolution of any given goal. Additionally, the nature of the ILCs problem to want to send the `input' and `state' to zero make it such that we must add extra exploration terms when constructing such large matrices, otherwise we end up with ill-conditioned matrices from which a controller cannot be extracted. It is thus desirable to find a way to reduce our input-outputs signals into a lower dimensions for the learning process.